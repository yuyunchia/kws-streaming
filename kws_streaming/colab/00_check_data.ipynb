{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "00_check-data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuyunchia/kws-streaming/blob/main/kws_streaming/colab/00_check_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB6qenKgVyIG"
      },
      "source": [
        "Copyright 2019 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "https://www.apache.org/licenses/LICENSE-2.0\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpPQDKFaVyII"
      },
      "source": [
        "Make sure that jupyter is installed by running below command (it will allow to create folders in user dir):\n",
        "\n",
        "```shell\n",
        "pip install jupyter --user\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxTAl8aSPKUG"
      },
      "source": [
        "## Set up environment\n",
        "## from kws_experiments_12_labels.md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWRFVys6Qq92"
      },
      "source": [
        "Set main folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJ1xcHH4PZU1",
        "outputId": "968fe110-f78f-4269-d826-efea46147d51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# create main folder\n",
        "%mkdir test\n",
        "\n",
        "# set path to a main folder\n",
        "!KWS_PATH=$PWD/test\n",
        "\n",
        "%cd $KWS_PATH\n",
        "# copy content of kws_streaming to a folder /tmp/test/kws_streaming\n",
        "!git clone https://github.com/google-research/google-research.git\n",
        "%mv google-research/kws_streaming .\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Install tensorflow with deps.\n",
        "\n",
        "# set up virtual env\n",
        "!pip install virtualenv\n",
        "!virtualenv --system-site-packages -p python3 ./venv3\n",
        "!source ./venv3/bin/activate\n",
        "\n",
        "# install TensorFlow, correct TensorFlow version is important\n",
        "!pip install --upgrade pip\n",
        "!pip install tf_nightly\n",
        "!pip install tensorflow_addons\n",
        "!pip install tensorflow_model_optimization\n",
        "# was tested on tf_nightly-2.3.0.dev20200515-cp36-cp36m-manylinux2010_x86_64.whl\n",
        "\n",
        "# install libs:\n",
        "!pip install pydot\n",
        "!pip install graphviz\n",
        "!pip install numpy\n",
        "!pip install absl-py\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Set up data sets:\n",
        "# download and set up path to data set V2 and set it up\n",
        "!wget https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz\n",
        "%mkdir data2\n",
        "%mv ./speech_commands_v0.02.tar.gz ./data2\n",
        "%cd ./data2\n",
        "!tar -xf ./speech_commands_v0.02.tar.gz\n",
        "%cd ../\n",
        "\n",
        "# path to data sets V2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Set path to models:\n",
        "# set up path for model training\n",
        "%mkdir $KWS_PATH/models_data_v2_12_labels\n",
        "\n",
        "# models trained on data V2\n",
        "!MODELS_PATH=$KWS_PATH/models_data_v2_12_labels\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Models training and evaluation:\n",
        "\n",
        "# CMD_TRAIN=\"bazel run -c opt --copt=-mavx2 kws_streaming/train:model_train_eval --\"\n",
        "!CMD_TRAIN=\"python -m kws_streaming.train.model_train_eval\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '$KWS_PATH'\n",
            "/content/data2\n",
            "Cloning into 'google-research'...\n",
            "remote: Enumerating objects: 37969, done.\u001b[K\n",
            "remote: Counting objects: 100% (2656/2656), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1697/1697), done.\u001b[K\n",
            "^C\n",
            "mv: cannot stat 'google-research/kws_streaming': No such file or directory\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/main.py\", line 68, in main\n",
            "    command = create_command(cmd_name, isolated=(\"--isolated\" in cmd_args))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/__init__.py\", line 109, in create_command\n",
            "    module = importlib.import_module(module_path)\n",
            "  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py\", line 14, in <module>\n",
            "    from pip._internal.cli.req_command import (\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/req_command.py\", line 20, in <module>\n",
            "    from pip._internal.index.collector import LinkCollector\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/index/collector.py\", line 27, in <module>\n",
            "    from pip._vendor import html5lib, requests\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/html5lib/__init__.py\", line 25, in <module>\n",
            "    from .html5parser import HTMLParser, parse, parseFragment\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/html5lib/html5parser.py\", line 6, in <module>\n",
            "    from . import _inputstream\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/html5lib/_inputstream.py\", line 33, in <module>\n",
            "    \"]\")\n",
            "  File \"/usr/lib/python3.7/re.py\", line 236, in compile\n",
            "    return _compile(pattern, flags)\n",
            "  File \"/usr/lib/python3.7/re.py\", line 288, in _compile\n",
            "    p = sre_compile.compile(pattern, flags)\n",
            "  File \"/usr/lib/python3.7/sre_compile.py\", line 764, in compile\n",
            "    p = sre_parse.parse(p, flags)\n",
            "  File \"/usr/lib/python3.7/sre_parse.py\", line 924, in parse\n",
            "    p = _parse_sub(source, pattern, flags & SRE_FLAG_VERBOSE, 0)\n",
            "  File \"/usr/lib/python3.7/sre_parse.py\", line 420, in _parse_sub\n",
            "    not nested and not items))\n",
            "  File \"/usr/lib/python3.7/sre_parse.py\", line 532, in _parse\n",
            "    if set and this in '-&~|' and source.next == this:\n",
            "KeyboardInterrupt\n",
            "created virtual environment CPython3.7.12.final.0-64 in 239ms\n",
            "  creator CPython3Posix(dest=/content/data2/venv3, clear=False, no_vcs_ignore=False, global=True)\n",
            "  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv)\n",
            "    added seed packages: pip==21.3.1, setuptools==58.3.0, wheel==0.37.0\n",
            "  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.3.1)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
            "Requirement already satisfied: tf_nightly in /usr/local/lib/python3.7/dist-packages (2.8.0.dev20211101)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tf_nightly) (1.19.5)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tf_nightly) (3.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tf_nightly) (1.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tf_nightly) (1.15.0)\n",
            "Requirement already satisfied: tb-nightly~=2.7.0.a in /usr/local/lib/python3.7/dist-packages (from tf_nightly) (2.7.0a20211013)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tf_nightly) (0.12.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tf_nightly) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tf_nightly) (3.7.4.3)\n",
            "Requirement already satisfied: keras-nightly~=2.8.0.dev in /usr/local/lib/python3.7/dist-packages (from tf_nightly) (2.8.0.dev2021110107)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tf_nightly) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tf_nightly) (0.21.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tf_nightly) (12.0.0)\n",
            "Requirement already satisfied: tf-estimator-nightly~=2.8.0.dev in /usr/local/lib/python3.7/dist-packages (from tf_nightly) (2.8.0.dev2021110108)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tf_nightly) (0.37.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tf_nightly) (3.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tf_nightly) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tf_nightly) (1.12)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tf_nightly) (1.41.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tf_nightly) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tf_nightly) (3.17.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tf_nightly) (1.12.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tf_nightly) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.7.0.a->tf_nightly) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.7.0.a->tf_nightly) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.7.0.a->tf_nightly) (0.6.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.7.0.a->tf_nightly) (57.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.7.0.a->tf_nightly) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.7.0.a->tf_nightly) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.7.0.a->tf_nightly) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.7.0.a->tf_nightly) (1.8.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.7.0.a->tf_nightly) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.7.0.a->tf_nightly) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.7.0.a->tf_nightly) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.7.0.a->tf_nightly) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tb-nightly~=2.7.0.a->tf_nightly) (4.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.7.0.a->tf_nightly) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.7.0.a->tf_nightly) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.7.0.a->tf_nightly) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.7.0.a->tf_nightly) (1.24.3)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tb-nightly~=2.7.0.a->tf_nightly) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.7.0.a->tf_nightly) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tb-nightly~=2.7.0.a->tf_nightly) (3.6.0)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.7/dist-packages (0.14.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: tensorflow_model_optimization in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
            "Requirement already satisfied: numpy~=1.14 in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_optimization) (1.19.5)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_optimization) (0.1.6)\n",
            "Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_optimization) (1.15.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.7/dist-packages (1.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.7/dist-packages (from pydot) (2.4.7)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (0.10.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py) (1.15.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "--2021-11-02 06:24:35--  https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.70.128, 74.125.132.128, 74.125.69.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.70.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2428923189 (2.3G) [application/gzip]\n",
            "Saving to: ‘speech_commands_v0.02.tar.gz.1’\n",
            "\n",
            "speech_commands_v0. 100%[===================>]   2.26G  38.4MB/s    in 27s     \n",
            "\n",
            "2021-11-02 06:25:02 (86.7 MB/s) - ‘speech_commands_v0.02.tar.gz.1’ saved [2428923189/2428923189]\n",
            "\n",
            "/content/data2/data2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVl4VQQUPZg9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0erap1CzPZsd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYby7eHhPZ2g"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9ktDknQPUE3"
      },
      "source": [
        "## From 00_check.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5or4KeYDta5v"
      },
      "source": [
        "!pip install jupyter --user"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGYS0NKeVyIJ"
      },
      "source": [
        "## !git clone https://github.com/google-research/google-research.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LutDR7qoVyIQ"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import tarfile\n",
        "import urllib\n",
        "\n",
        "## from 01_train\n",
        "import zipfile\n",
        "\n",
        "\n",
        "sys.path.append('./google-research')\n",
        "\n",
        "## some change\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8e_WfgaVyIV"
      },
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # no need to use gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vxfxt-VVyIa"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf1\n",
        "import logging\n",
        "from kws_streaming.models import model_flags\n",
        "from kws_streaming.models import models\n",
        "from kws_streaming.layers.modes import Modes\n",
        "from kws_streaming.train import test\n",
        "from kws_streaming.models import utils\n",
        "from kws_streaming.data import input_data\n",
        "from kws_streaming.data import input_data_utils as du\n",
        "from kws_streaming.models import model_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIkBmOt6VyIe"
      },
      "source": [
        "tf1.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSETc8rdVyIl"
      },
      "source": [
        "config = tf1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "sess = tf1.Session(config=config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnvm8YMlVyIq"
      },
      "source": [
        "tf1.disable_eager_execution()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjvGL2IgVyIw"
      },
      "source": [
        "DATA_VERSION = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smFll0s2VyI0"
      },
      "source": [
        "current_dir = os.getcwd()\n",
        "\n",
        "if DATA_VERSION == 2:\n",
        "  DATA_URL = \"https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz\"\n",
        "  DATA_PATH = os.path.join(current_dir, \"data2/\")\n",
        "elif DATA_VERSION == 1:\n",
        "  DATA_URL = \"http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz\"\n",
        "  DATA_PATH = os.path.join(current_dir, \"data1/\")\n",
        "else:\n",
        "  assert(False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ap7kbKPVyI4"
      },
      "source": [
        "DATA_PATH"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCFAzvyfVyI8"
      },
      "source": [
        "# create folder in current dir.\n",
        "# not data will be downloaded in DATA_PATH, feel free to specify your own DATA_PATH\n",
        "os.makedirs(DATA_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1232Ifp3VyI_"
      },
      "source": [
        "base_name = os.path.basename(DATA_URL)\n",
        "base_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kf5YF4SsVyJD"
      },
      "source": [
        "# it can take some time to download 2.3GB. After unpacking total size is 5.4GB\n",
        "arch_file_name = os.path.join(DATA_PATH, base_name)\n",
        "if not os.path.isfile(arch_file_name):\n",
        "  # download data\n",
        "  if sys.version_info >= (2, 5):\n",
        "    file_path = urllib.request.urlretrieve(DATA_URL, filename=arch_file_name)[0]\n",
        "  else:\n",
        "    file_path = urllib.urlretrieve(DATA_URL, filename=arch_file_name)[0]\n",
        "\n",
        "  # unpack it\n",
        "  file_name, file_extension = os.path.splitext(base_name)\n",
        "  tar = tarfile.open(file_path)\n",
        "  tar.extractall(DATA_PATH)\n",
        "  tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8z0-SnfVyJG"
      },
      "source": [
        "# default parameters for data splitting\n",
        "flags = model_params.Params()\n",
        "flags.data_dir = DATA_PATH\n",
        "flags.data_url = DATA_URL\n",
        "flags = model_flags.update_flags(flags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsRktp1mVyJK"
      },
      "source": [
        "audio_processor = input_data.AudioProcessor(flags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXABxi76VyJO"
      },
      "source": [
        "testing_set_size = audio_processor.set_size('testing')\n",
        "print(\"testing_set_size \" + str(testing_set_size))\n",
        "training_set_size = audio_processor.set_size('training')\n",
        "print(\"training_set_size \" + str(training_set_size))\n",
        "validation_set_size = audio_processor.set_size('validation')\n",
        "print(\"validation_set_size \" + str(validation_set_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDC0kbR2VyJR"
      },
      "source": [
        "# V2\n",
        "# testing_set_size 4890\n",
        "# training_set_size 36923\n",
        "# validation_set_size 4445\n",
        "\n",
        "# V1\n",
        "# testing_set_size 3081\n",
        "# training_set_size 22246\n",
        "# validation_set_size 3093"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RocIfpAtVyJU"
      },
      "source": [
        "# all words used for modeling: we have target words + unknown words (with index 1)\n",
        "audio_processor.word_to_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iea72Q5oVyJY"
      },
      "source": [
        "# find the start of the file name where label begins\n",
        "string = audio_processor.data_index[\"validation\"][0]['file']\n",
        "res = [i for i in range(len(string)) if string.startswith('/', i)] \n",
        "start_file = res[-2]+1\n",
        "start_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQWh0HF0VyJb"
      },
      "source": [
        "audio_processor.data_index[\"validation\"][0]['file'][start_file:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdihI451VyJe"
      },
      "source": [
        "index_to_label = {}\n",
        "unknown_category = []\n",
        "# labeles used for training\n",
        "for word in audio_processor.word_to_index.keys():\n",
        "  if audio_processor.word_to_index[word] == du.SILENCE_INDEX:\n",
        "    index_to_label[audio_processor.word_to_index[word]] = du.SILENCE_LABEL\n",
        "  elif audio_processor.word_to_index[word] == du.UNKNOWN_WORD_INDEX:\n",
        "    index_to_label[audio_processor.word_to_index[word]] = du.UNKNOWN_WORD_LABEL\n",
        "    unknown_category.append(word)\n",
        "  else:\n",
        "    index_to_label[audio_processor.word_to_index[word]] = word\n",
        "\n",
        "# training labels\n",
        "index_to_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsgt_OpnVyJh"
      },
      "source": [
        "# words belonging to unknown categry\n",
        "unknown_category"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDCSwc9SVyJk"
      },
      "source": [
        "def get_distribution(mode):\n",
        "  distrib_label = {}\n",
        "  distrib_words = {}\n",
        "  files = {}\n",
        "  for data in audio_processor.data_index[mode]:\n",
        "    word = data['label']\n",
        "    file = data['file'][start_file:]\n",
        "    index = audio_processor.word_to_index[word]\n",
        "    label = index_to_label[index]\n",
        "    if word in files:\n",
        "      files[word].append(file)\n",
        "    else:\n",
        "      files[word] = [file]\n",
        "\n",
        "    if label in distrib_label:\n",
        "      distrib_label[label] = distrib_label[label] + 1\n",
        "    else:\n",
        "      distrib_label[label] = 0\n",
        "\n",
        "    if word in distrib_words:\n",
        "      distrib_words[word] = distrib_words[word] + 1\n",
        "    else:\n",
        "      distrib_words[word] = 0\n",
        "  return distrib_words, distrib_label, files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwrCIcC0VyJn"
      },
      "source": [
        "# distribution of labeles in testing data\n",
        "distrib_words_testing, distrib_labels_testing, files_testing = get_distribution('testing')\n",
        "distrib_labels_testing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qG0cJ1ifVyJr"
      },
      "source": [
        "# distribution of labeles in training data\n",
        "distrib_words_training, distrib_labels_training, files_training = get_distribution('training')\n",
        "distrib_labels_training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRsumUc0VyJv"
      },
      "source": [
        "def parse_files(set_list_fname, label='yes'):\n",
        "  set_files = []\n",
        "  with open(set_list_fname) as f:\n",
        "    while True:\n",
        "      line = f.readline()\n",
        "      if not line:\n",
        "        break\n",
        "      if line.split('/')[0] == label:\n",
        "        set_files.append(line[:-1])\n",
        "  return set_files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHw_eD2rVyJy"
      },
      "source": [
        "def validate(my_list1, list2, print_in_list2=False):\n",
        "  cnt_my_val2 = 0\n",
        "  cnt_my_val1 = 0\n",
        "  for my_val in my_list1:\n",
        "    if my_val in list2:\n",
        "      cnt_my_val2 = cnt_my_val2 + 1\n",
        "      if print_in_list2:\n",
        "        print(my_val)\n",
        "    else:\n",
        "      cnt_my_val1 = cnt_my_val1 + 1\n",
        "      if not print_in_list2:\n",
        "        print(my_val)\n",
        "  return cnt_my_val1, cnt_my_val2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSbC4wmmVyJ2"
      },
      "source": [
        "file_list = os.path.join(DATA_PATH, \"testing_list.txt\")\n",
        "\n",
        "# validate that all wav used during testing belongs to testing_list\n",
        "for word in files_testing.keys():\n",
        "  if word != '_silence_':\n",
        "    word_files = parse_files(file_list, label=word)\n",
        "    _, cnt_val = validate(files_testing[word], word_files, False)\n",
        "    assert(cnt_val-len(files_testing[word])==0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Q8zKJWwVyJ4"
      },
      "source": [
        "\n",
        "distrib_words_training, distrib_labels_training, files_training = get_distribution('training')\n",
        "\n",
        "# validate that all wav used during testing do not belong to training data\n",
        "for word in files_testing.keys():\n",
        "  if word != '_silence_': # silence file does not matter becasue it is multiplied by zero\n",
        "    word_files = files_testing[word]\n",
        "    _, cnt_val = validate(files_training[word], word_files, True)\n",
        "    assert(cnt_val == 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQqB8IqLVyJ7"
      },
      "source": [
        "##\n",
        "## below is from 01_train.ipynb\n",
        "##\n",
        "##\n",
        "##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I17NZafUuR0q"
      },
      "source": [
        "# TF streaming\n",
        "from kws_streaming.models import models\n",
        "from kws_streaming.models import utils\n",
        "from kws_streaming.layers.modes import Modes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dV6STWZIuT6p"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf1\n",
        "import logging\n",
        "from kws_streaming.models import model_flags\n",
        "from kws_streaming.models import model_params\n",
        "from kws_streaming.train import test\n",
        "from kws_streaming.train import train\n",
        "from kws_streaming import data\n",
        "tf1.disable_eager_execution()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZnb2T0lubdg"
      },
      "source": [
        "config = tf1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "sess = tf1.Session(config=config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5kHjBtiueRA"
      },
      "source": [
        "# general imports\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import scipy as scipy\n",
        "import scipy.io.wavfile as wav\n",
        "import scipy.signal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl6fmayDuhzu"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oO30qMQujxS"
      },
      "source": [
        "tf1.reset_default_graph()\n",
        "sess = tf1.Session()\n",
        "tf1.keras.backend.set_session(sess)\n",
        "tf1.keras.backend.set_learning_phase(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuNsC0UMuks-"
      },
      "source": [
        "# **Set path to data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPNUhbiZutYo"
      },
      "source": [
        "# set PATH to data sets (for example to speech commands V2):\n",
        "# it can be downloaded from\n",
        "# https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz\n",
        "# if you already run \"00_check-data.ipynb\" then folder \"data2\" should be located in the current dir\n",
        "current_dir = os.getcwd()\n",
        "DATA_PATH = os.path.join(current_dir, \"data2/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkcsiWBgunGa"
      },
      "source": [
        "def waveread_as_pcm16(filename):\n",
        "  \"\"\"Read in audio data from a wav file.  Return d, sr.\"\"\"\n",
        "  samplerate, wave_data = wav.read(filename)\n",
        "  # Read in wav file.\n",
        "  return wave_data, samplerate\n",
        "\n",
        "def wavread_as_float(filename, target_sample_rate=16000):\n",
        "  \"\"\"Read in audio data from a wav file.  Return d, sr.\"\"\"\n",
        "  wave_data, samplerate = waveread_as_pcm16(filename)\n",
        "  desired_length = int(\n",
        "          round(float(len(wave_data)) / samplerate * target_sample_rate))\n",
        "  wave_data = scipy.signal.resample(wave_data, desired_length)\n",
        "\n",
        "  # Normalize short ints to floats in range [-1..1).\n",
        "  data = np.array(wave_data, np.float32) / 32768.0\n",
        "  return data, target_sample_rate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LJpAmyYu5uY"
      },
      "source": [
        "# Set path to wav file to visualize it\n",
        "wav_file = os.path.join(DATA_PATH, \"left/012187a4_nohash_0.wav\")\n",
        "\n",
        "# read audio file\n",
        "wav_data, samplerate = wavread_as_float(wav_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Kazp0W0u7qB"
      },
      "source": [
        "assert samplerate == 16000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwPBvbx5u9eD"
      },
      "source": [
        "plt.plot(wav_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yquof2BKvCPd"
      },
      "source": [
        "# **Set path to a model with config**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "842Aa7cNvLth"
      },
      "source": [
        "# select model name should be one of\n",
        "model_params.HOTWORD_MODEL_PARAMS.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCOnBK3wvRqi"
      },
      "source": [
        "# This notebook is configured to work with 'ds_tc_resnet' and 'svdf'.\n",
        "MODEL_NAME = 'ds_tc_resnet'\n",
        "# MODEL_NAME = 'svdf'\n",
        "MODELS_PATH = os.path.join(current_dir, \"models\")\n",
        "MODEL_PATH = os.path.join(MODELS_PATH, MODEL_NAME + \"/\")\n",
        "MODEL_PATH"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeUJbS1KvTjT"
      },
      "source": [
        "os.makedirs(MODEL_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXZ1mP1tvVh5"
      },
      "source": [
        "# get toy model settings\n",
        "FLAGS = model_params.HOTWORD_MODEL_PARAMS[MODEL_NAME]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTN6OdlyvXam"
      },
      "source": [
        "# set path to data and model (where model will be stored)\n",
        "FLAGS.data_dir = DATA_PATH\n",
        "FLAGS.train_dir = MODEL_PATH\n",
        "\n",
        "# set speech feature extractor properties\n",
        "FLAGS.mel_upper_edge_hertz = 7600\n",
        "FLAGS.window_size_ms = 30.0\n",
        "FLAGS.window_stride_ms = 10.0\n",
        "FLAGS.mel_num_bins = 80\n",
        "FLAGS.dct_num_features = 40\n",
        "FLAGS.feature_type = 'mfcc_tf'\n",
        "FLAGS.preprocess = 'raw'\n",
        "\n",
        "# for numerical correctness of streaming and non streaming models set it to 1\n",
        "# but for real use case streaming set it to 0\n",
        "FLAGS.causal_data_frame_padding = 0\n",
        "\n",
        "FLAGS.use_tf_fft = True\n",
        "FLAGS.mel_non_zero_only = not FLAGS.use_tf_fft\n",
        "\n",
        "# set training settings\n",
        "FLAGS.train = 1\n",
        "# reduced number of training steps for test only\n",
        "# so model accuracy will be low,\n",
        "# to improve accuracy set how_many_training_steps = '40000,40000,20000,20000'\n",
        "FLAGS.how_many_training_steps = '400,400,400,400'\n",
        "FLAGS.learning_rate = '0.001,0.0005,0.0001,0.00002'\n",
        "FLAGS.lr_schedule = 'linear'\n",
        "FLAGS.verbosity = logging.INFO\n",
        "\n",
        "# data augmentation parameters\n",
        "FLAGS.resample = 0.15\n",
        "FLAGS.time_shift_ms = 100\n",
        "FLAGS.use_spec_augment = 1\n",
        "FLAGS.time_masks_number = 2\n",
        "FLAGS.time_mask_max_size = 25\n",
        "FLAGS.frequency_masks_number = 2\n",
        "FLAGS.frequency_mask_max_size = 7\n",
        "FLAGS.pick_deterministically = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPBW-Dsgvh9D"
      },
      "source": [
        "FLAGS.model_name = MODEL_NAME\n",
        "\n",
        "# model parameters are different for every model\n",
        "if MODEL_NAME == 'svdf':\n",
        "  FLAGS.model_name = MODEL_NAME\n",
        "  FLAGS.svdf_memory_size = \"4,10,10,10,10,10\"\n",
        "  FLAGS.svdf_units1 = \"16,32,32,32,64,128\"\n",
        "  FLAGS.svdf_act = \"'relu','relu','relu','relu','relu','relu'\"\n",
        "  FLAGS.svdf_units2 = \"40,40,64,64,64,-1\"\n",
        "  FLAGS.svdf_dropout = \"0.0,0.0,0.0,0.0,0.0,0.0\"\n",
        "  FLAGS.svdf_pad = 0\n",
        "  FLAGS.dropout1 = 0.0\n",
        "  FLAGS.units2 = ''\n",
        "  FLAGS.act2 = ''\n",
        "elif MODEL_NAME == 'ds_tc_resnet':\n",
        "  # it is an example of model streaming with strided convolution, strided pooling and dilated convolution\n",
        "  FLAGS.activation = 'relu'\n",
        "  FLAGS.dropout = 0.0\n",
        "  FLAGS.ds_filters = '128, 64, 64, 64, 128, 128'\n",
        "  FLAGS.ds_filter_separable = '1, 1, 1, 1, 1, 1'\n",
        "  FLAGS.ds_repeat = '1, 1, 1, 1, 1, 1'\n",
        "  FLAGS.ds_residual = '0, 1, 1, 1, 0, 0' # residual can not be applied with stride\n",
        "#   FLAGS.ds_kernel_size = '11, 5, 15, 7, 29, 1'\n",
        "  FLAGS.ds_kernel_size = '11, 5, 15, 17, 15, 1'\n",
        "  FLAGS.ds_dilation = '1, 1, 1, 1, 2, 1'\n",
        "  FLAGS.ds_stride = '1, 1, 1, 1, 1, 1'\n",
        "  FLAGS.ds_pool = '1, 2, 1, 1, 1, 1'\n",
        "  # model should be causal, so that we can covert it to streaming mode\n",
        "  # if model is non causal then all non causal components should use Delay layer\n",
        "  FLAGS.ds_padding = \"'causal', 'causal', 'causal', 'causal', 'causal', 'causal'\"\n",
        "else:\n",
        "  raise ValueError(\"set parameters for other models\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9UygW-Mvlel"
      },
      "source": [
        "flags = model_flags.update_flags(FLAGS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnO7WV4Qvm_r"
      },
      "source": [
        "with open(os.path.join(flags.train_dir, 'flags.json'), 'wt') as f:\n",
        "  json.dump(flags.__dict__, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUYFDMTHvosp"
      },
      "source": [
        "# visualize a model\n",
        "model_non_stream_batch = models.MODELS[flags.model_name](flags)\n",
        "tf.keras.utils.plot_model(\n",
        "    model_non_stream_batch,\n",
        "    show_shapes=True,\n",
        "    show_layer_names=True,\n",
        "    expand_nested=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMBl3d9cvqZu"
      },
      "source": [
        "model_non_stream_batch.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4z-uxCnKvsFo"
      },
      "source": [
        "# **Model training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6McwKLnvybj"
      },
      "source": [
        "# Model training\n",
        "train.train(flags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iMeZ-eYv7eH"
      },
      "source": [
        "## **Run model evaluation**\n",
        "### TF **Run non streaming inference**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRlhrJsYwFCY"
      },
      "source": [
        "folder_name = 'tf'\n",
        "test.tf_non_stream_model_accuracy(flags, folder_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsB71EGuwIeE"
      },
      "source": [
        "more testing functions can be found at [test](https://github.com/google-research/google-research/blob/master/kws_streaming/train/test.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huWz_40nwQLl"
      },
      "source": [
        "##\n",
        "## below is from 02_inference.ipynb\n",
        "##"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}